{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= start =========\n",
      "(5.1933475857104945, 130)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't pickle cKDTree objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-19e7af91f979>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;31m# a = cPickle.dumps(kdt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mkdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkdt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/copy_reg.pyc\u001b[0m in \u001b[0;36m_reduce_ex\u001b[1;34m(self, proto)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"can't pickle %s objects\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't pickle cKDTree objects"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "# from scipy.spatial import cKDTree \n",
    "from scipy.spatial import cKDTree \n",
    "from scipy.spatial import kdtree\n",
    "import pyspark\n",
    "import cPickle\n",
    "\n",
    "\n",
    "def getNeighbours(point):\n",
    "    kdt_1 = kdt_b.value\n",
    "    return (kdt_1.data)\n",
    "#     return kdt_1.query_ball_point(point, r=(window_size/2.0),p=2)\n",
    "#     kdt_1 = kdtree.KDTree(cloud_b.value[:,0:2])\n",
    "#     return type(kdt_1)\n",
    "#     return type(kdt_b.value)#.value.n)\n",
    "#     return kdt_1.data\n",
    "#     return point\n",
    "#     return kdt_1.query_ball_point(point, r=(window_size/2.0),p=2)\n",
    "       \n",
    "    \n",
    "# Build different windows and height thresholds\n",
    "windows = []\n",
    "height_thresholds = []\n",
    "\n",
    "# Default parameters\n",
    "slope = 1.0\n",
    "cell_size = 1.0\n",
    "base = 2.0\n",
    "max_distance = 1\n",
    "initial_distance = 0.5\n",
    "max_window_size = 20\n",
    "window_type = 'linear'\n",
    "\n",
    "window_size = 0.0\n",
    "i = 0\n",
    "while window_size < max_window_size:\n",
    "    # Create different windows\n",
    "    if window_type == 'linear':\n",
    "        window_size = cell_size * ((2*(i+1)*base) + 1)\n",
    "    elif window_type == 'exponential':\n",
    "        window_size = cell_size * ((2*base**(i+1) + 1))\n",
    "    if window_size > max_window_size:\n",
    "        break\n",
    "    windows.append(window_size)\n",
    "    # Create corresponding height threshold\n",
    "    if i == 0:\n",
    "        height_threshold = initial_distance\n",
    "    elif height_threshold > max_distance:\n",
    "        height_threshold = max_distance\n",
    "    else:\n",
    "        height_threshold = slope*(windows[-1]-windows[-2])*cell_size + initial_distance\n",
    "\n",
    "\n",
    "    height_thresholds.append(height_threshold)\n",
    "    i += 1\n",
    "\n",
    "print \"========= start =========\"\n",
    "# file_name = \"odm_mesh_small_no_outliers.xyz\"\n",
    "file_name = \"odm_mesh_small_no_outliers2.xyz\" \n",
    "# file_name = \"odm_mesh_small.xyz\" \n",
    "start_time = time.time()\n",
    "with open(file_name, 'rb') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=' ')\n",
    "    pointcloud = [map(float, row) for row in csvreader]\n",
    "np_pointcloud = np.array(pointcloud)\n",
    "# kdt = cKDTree(np_pointcloud[:,0:2])\n",
    "kdt = cKDTree(np_pointcloud[:,0:2])\n",
    "\n",
    "\n",
    "# a = cPickle.dumps(kdt)\n",
    "print kdt.query([-150,-50])\n",
    "a = cPickle.dumps(kdt)\n",
    "b = cPickle.loads(a)\n",
    "print b.query([-150,-50])\n",
    "\n",
    "# print kdt.data\n",
    "# kdt = kdtree.KDTree(np_pointcloud[:,0:2])\n",
    "\n",
    "# kdt_b = sc.broadcast(cPickle.dumps(kdt))\n",
    "\n",
    "# kdt_serialized = sc.serializer.dumps(kdt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# kdt_serialized = pyspark.serializers.PickleSerializer.dumps(kdt)\n",
    "cloud_b = sc.broadcast(np_pointcloud)\n",
    "kdt_b = sc.broadcast(kdt)\n",
    "wind_b = sc.broadcast(window_size)\n",
    "\n",
    "\n",
    "kdt_u = kdt_b.value.data\n",
    "print kdt_u\n",
    "\n",
    "for pt_idx, point in enumerate(np_pointcloud[0:10000,0:2]):\n",
    "#     print pt_idx\n",
    "#     print type(point)\n",
    "    pt_neighbours = kdt.query_ball_point(point, r=(window_size/2.0),p=2)\n",
    "#     print pt_neighbours\n",
    "\n",
    "np_indices = np.arange(len(np_pointcloud))\n",
    "z_vals = [p[2] for p in np_pointcloud[np_indices]]\n",
    "points = sc.textFile(file_name)\n",
    "points_split = points.map(lambda x: (x.split(\" \")))\n",
    "points_parsed = points_split.map(lambda line: [float(line[0]), float(line[1]), float(line[2])])\n",
    "\n",
    "print \"========= erosion =========\"\n",
    "\n",
    "# getn = (lambda x: kdt.query_ball_point(x[0:2], r=(window_size/2.0),p=2))\n",
    "# t =  \n",
    "# t = points_parsed.\n",
    "# t =  points_parsed.map(getn)\n",
    "# print t.take(3)\n",
    "\n",
    "# t = points_parsed.map(lambda x: kdt_b.value.query_ball_point(x, r=(window_size/2.0),p=2))\n",
    "# t = points_parsed.map(lambda x: getNeighbours(x))\n",
    "t = points_parsed.map(lambda x: getNeighbours(x))\n",
    "# t = points_parsed.map(lambda x: kdt.query_ball_point([x[0],x[1]], r=50, p=2))\n",
    "# print 'test point: ', kdt.query_ball_point([-255,-55], r=50, p=2)\n",
    "# print kdt.data\n",
    "print t.take(5)\n",
    "\n",
    "breakdsadas\n",
    "\n",
    "test = points_parsed.map(lambda x: ((x[0],x[1]),getNeighbours(x))).mapValues(min).map(lambda (x,y): [x[0],x[1],y])\n",
    "pointcloud_eroded = np.array(test.collect())\n",
    "\n",
    "print \"========= dialation =========\"\n",
    "\n",
    "pointcloud_dialated = points_parsed.map(lambda x: ((x[0],x[1]), getNeighbours(x))).mapValues(max)#.map(lambda (x,y): [x[0],x[1],y])\n",
    "\n",
    "window_size = windows[0]\n",
    "thres = height_thresholds[0]\n",
    "\n",
    "print \"========= flags =========\"\n",
    "points_parsed_tuple = points_parsed.map(lambda x: ((x[0], x[1]), x[2]))\n",
    "print \"length points_parsed_tuple \", points_parsed_tuple.count(), \"\\n\"\n",
    "print \"length pointcloud_dialated \", pointcloud_dialated.count(), \"\\n\"\n",
    "\n",
    "flags = points_parsed_tuple.cogroup(pointcloud_dialated)\n",
    "# .map(lambda (x,y): (x,(list(y[0])[0],list(y[1])[0])) if(list(y[0])[0] != \"\" and list(y[1])[0] != \"\") else None)\n",
    "# flags = flags.map(lambda (key, tuple): (key,1) if(abs(tuple[0] - tuple[1]) > thres) else (key,0))\n",
    "print \"========= \", time.time()-start_time ,\" seconds =========\"\n",
    "print flags.take(1)\n",
    "print flags.collect()\n",
    "\n",
    "\n",
    "\n",
    "toToFail\n",
    "    \n",
    "window_size = windows[0]\n",
    "points_matrix = points_enum.cartesian(points_enum)\n",
    "new_points = points_matrix.map(lambda (x,y): ((x[1][0], x[1][1]), y[1][2]) if ( y[1][0] >= x[1][0] - window_size/2 and y[1][0] <= x[1][0] + window_size/2 and y[1][1] >= x[1][1] - window_size/2 and y[1][1] <= x[1][1] + window_size/2 ) else None)\n",
    "erosion_p = new_points.filter(lambda x: x!= None).reduceByKey(min)\n",
    "erosion_p = erosion_p.map(lambda (key,z): [key[0], key[1], z])\n",
    "print \"\\nErosion\\n\",erosion_p.take(5),\"\\n\"\n",
    "\n",
    "points_enum_2 = erosion_p.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "points_matrix_2 = points_enum_2.cartesian(points_enum_2)\n",
    "new_points_2 = points_matrix_2.map(lambda (x,y): ((x[1][0], x[1][1]), y[1][2]) if ( y[1][0] >= x[1][0] - window_size/2 and y[1][0] <= x[1][0] + window_size/2 and y[1][1] >= x[1][1] - window_size/2 and y[1][1] <= x[1][1] + window_size/2 ) else None)\n",
    "dialation_p = new_points_2.filter(lambda x: x!= None).reduceByKey(max)\n",
    "\n",
    "points_parsed_tuple = points_parsed.map(lambda x: ((x[0], x[1]), x[2]))\n",
    "flags = points_parsed_tuple.cogroup(dialation_p).map(lambda (x,y): (x,(list(y[0])[0],list(y[1])[0])) if(list(y[0])[0] != \"\" and list(y[1])[0] != \"\") else None)\n",
    "\n",
    "print \"\\ndone\\n\", flags.take(15), \"\\n\"\n",
    "\n",
    "thres = height_thresholds[0]\n",
    "flags = flags.map(lambda (key, tuple): (key,1) if(abs(tuple[0] - tuple[1]) > thres) else (key,0))\n",
    "\n",
    "print \"\\n\\nflags\\n\", flags.take(15), \"\\n\"\n",
    "\n",
    "\n",
    "# sys.exit()\n",
    "\n",
    "# ARE WE ALLOWED TO DO THAT? IN OPEN_MP WE HAD TO TAKE CARE OF THIS\n",
    "i = 0 \n",
    "for window, thres in zip(windows, height_thresholds):\n",
    "    print \"i: \", i\n",
    "    if i > 0:\n",
    "        print \"\\n\\n================ iter ================\"\n",
    "        window_size = windows[i]\n",
    "        thres = height_thresholds[i]\n",
    "        \n",
    "        erosion_p = new_points.filter(lambda x: x!= None).reduceByKey(min)\n",
    "        erosion_p = erosion_p.map(lambda (key,z): [key[0], key[1], z])\n",
    "\n",
    "        points_enum_2 = erosion_p.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "        points_matrix_2 = points_enum_2.cartesian(points_enum_2)\n",
    "        new_points_2 = points_matrix_2.map(lambda (x,y): ((x[1][0], x[1][1]), y[1][2]) if ( y[1][0] >= x[1][0] - window_size/2 and y[1][0] <= x[1][0] + window_size/2 and y[1][1] >= x[1][1] - window_size/2 and y[1][1] <= x[1][1] + window_size/2 ) else None)\n",
    "        dialation_p = new_points_2.filter(lambda x: x!= None).reduceByKey(max)\n",
    "        \n",
    "        points_parsed_tuple = points_parsed.map(lambda x: ((x[0], x[1]), x[2]))\n",
    "        flags_new = points_parsed_tuple.cogroup(dialation_p).map(lambda (x,y): (x,(list(y[0])[0],list(y[1])[0])) if(list(y[0])[0] != \"\" and list(y[1])[0] != \"\") else None)\n",
    "        flags_new = flags_new.map(lambda (key, tuple): (key,1) if(abs(tuple[0] - tuple[1]) > thres) else (key,0))\n",
    "        flags = flags_new.cogroup(flags)\n",
    "        flags = flags.map(lambda (x,y): (x,(list(y[0])[0],list(y[1])[0]))).map(lambda (x,y): (x, (y[0] or y[1])))\n",
    "    i+=1\n",
    "print '\\n\\n===========finish============\\n', flags.map(lambda (x,y): y).count()\n",
    "    \n",
    "end_time = time.time()    \n",
    "print \"\\nExecution time: \", (end_time-start_time), \"sec\"\n",
    "# with open('dataset/pcloud1.xyz', 'wb') as csvfile:\n",
    "#     csvwriter = csv.writer(csvfile, delimiter=' ')\n",
    "#     csvwriter.writerows(np_pointcloud[np.where(flags == 1)[0], :].tolist())\n",
    "\n",
    "# with open('dataset/pcloud2.xyz', 'wb') as csvfile:\n",
    "#     csvwriter = csv.writer(csvfile, delimiter=' ')\n",
    "#     csvwriter.writerows(np_pointcloud[np.where(flags == 0)[0], :].tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
